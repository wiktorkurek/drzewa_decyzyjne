{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passing-square",
   "metadata": {},
   "source": [
    "# Lab. 2\n",
    "Wiktor Kurek, Dawid Stachowiak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-minutes",
   "metadata": {},
   "source": [
    "# Zadanie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threatened-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "print(load_breast_cancer().data)\n",
    "print(load_breast_cancer().target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer().data\n",
    "target = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-asthma",
   "metadata": {},
   "source": [
    "### 1. Wymiar wczytanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "realistic-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-victim",
   "metadata": {},
   "source": [
    "### 2. Ilość wartości unikatowych w wektorze target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparative-chosen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-brain",
   "metadata": {},
   "source": [
    "### 3. Zaproponuj usunięcie tych kolumn ze zbioru danych, które Twoim zdaniem dostarczają najmniej informacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "promising-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:,:20] #usuwam 10 ostatnich kolumn (worst) poniewaz dają nam informacje tylko o skrajnej wartości jednego pomairu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-ceramic",
   "metadata": {},
   "source": [
    "### 4. Zapisz w pliku dataset_cut.csv wypadkowy zbiór danych jak poniżej: ColName1; ColName2; …; TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facial-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 21)\n"
     ]
    }
   ],
   "source": [
    "features = load_breast_cancer().feature_names[:20]\n",
    "features = np.append (features, ['TARGET'])\n",
    "dataset = np.append(data, target.reshape(569,1), axis=1)\n",
    "print(dataset.shape)\n",
    "df = pd.DataFrame(data = dataset, columns = features)\n",
    "df.to_csv(\"dataset_cut.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-column",
   "metadata": {},
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-parks",
   "metadata": {},
   "source": [
    "### 1. Wprowadź zbiór danych do algorytmu PCA\n",
    "### 2. Wykonaj redukcję wymiarowości do 5 wymiarów wypadkowego zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "offensive-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.08351496  0.68905184 -1.97759299 -2.41601264  1.7881753 ]\n",
      " [ 1.13678036 -3.22466166 -0.42666245  0.38989184  0.7887873 ]\n",
      " [ 4.40230338 -1.46673681 -0.90834385 -0.18179909  0.03645464]\n",
      " ...\n",
      " [ 0.85472157 -1.47573878  0.66606694  2.09940824 -0.86371105]\n",
      " [ 7.41150964  0.11856378 -1.13090005  0.17313771 -2.64128003]\n",
      " [-4.09694916 -0.01290673  2.66523117  0.39004325 -0.39297542]]\n",
      "(569, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standarized = StandardScaler().fit_transform(data)\n",
    "pca = PCA(n_components = 5)\n",
    "reduced = pca.fit_transform(standarized)\n",
    "print(reduced)\n",
    "reduced_df = pd.DataFrame(reduced)\n",
    "print(reduced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-boundary",
   "metadata": {},
   "source": [
    "### 3. Zapisz w pliku dataset_pca_5.csv wypadkowy zbiór danych jak poniżej: COMP1; COMP2; COMP3; COMP4; COMP5; TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-mortgage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 6)\n"
     ]
    }
   ],
   "source": [
    "features = ['COMP1', 'COMP2', 'COMP3', 'COMP4', 'COMP5', 'TARGET']\n",
    "dataset = np.append(reduced, target.reshape(569,1), axis=1)\n",
    "print(dataset.shape)\n",
    "df = pd.DataFrame(data = dataset, columns = features)\n",
    "df.to_csv(\"dataset_pca_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-yorkshire",
   "metadata": {},
   "source": [
    "### 4. Oblicz wariancję zbioru danych po redukcji wymiarowości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "welcome-bracket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3868560510357937\n",
      "3.3880469286908705\n",
      "COMP1     8.745573\n",
      "COMP2     4.168247\n",
      "COMP3     1.718822\n",
      "COMP4     1.309718\n",
      "COMP5     1.021735\n",
      "TARGET    0.234177\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import statistics as stat\n",
    "print(reduced.var())\n",
    "#lub\n",
    "print(stat.variance(reduced.flatten()))\n",
    "print(df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-italic",
   "metadata": {},
   "source": [
    "### 5. Oblicz wartość wariancji wyjaśnionej dla wygenerowanych składowych (ang.explained variance ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "usual-functionality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43651014, 0.20804607, 0.08579006, 0.06537079, 0.05099695])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-indiana",
   "metadata": {},
   "source": [
    "### 6. Skomentuj otrzymane rezultaty.\n",
    "  \n",
    "  Wariancja całego zbioru po redukcji wymiarowości wzrosła wielokrotnie, co oznacza że po tej operacji zróżnicowanie cech w naszym zbiorze wzrosło."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-sender",
   "metadata": {},
   "source": [
    "# Zadanie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automotive-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = load_breast_cancer().data\n",
    "target_new = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "established-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_new[:,:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-conflict",
   "metadata": {},
   "source": [
    "### 1. Uzasadnij optymalny wymiar zbioru danych zakładając, że w zbiorze wypadkowym suma składowych będzie stanowiła minimum 90% wariancji.\n",
    "### 2. Wykonaj redukcję wymiarowości za pomocą algorytmu PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effective-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.08351496  0.68905184 -1.97759301 ...  1.78817534 -0.79021754\n",
      "  -1.74916791]\n",
      " [ 1.13678036 -3.22466166 -0.42666245 ...  0.78878731 -0.18664794\n",
      "   0.49650704]\n",
      " [ 4.40230338 -1.46673681 -0.90834384 ...  0.03645473 -0.31847824\n",
      "   0.55624229]\n",
      " ...\n",
      " [ 0.85472157 -1.47573878  0.66606695 ... -0.86371106  0.37005545\n",
      "  -0.0670664 ]\n",
      " [ 7.41150964  0.11856378 -1.13090005 ... -2.64128011 -0.39100681\n",
      "  -0.23455732]\n",
      " [-4.09694916 -0.01290673  2.66523116 ... -0.39297531 -0.94600428\n",
      "  -1.47397124]]\n",
      "(569, 7)\n"
     ]
    }
   ],
   "source": [
    "standarized_new = StandardScaler().fit_transform(data_new)\n",
    "pca_new = PCA(.90)\n",
    "reduced_new = pca_new.fit_transform(standarized_new)\n",
    "print(reduced_new)\n",
    "df_reduced = pd.DataFrame(data = reduced_new)\n",
    "print(df_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-blogger",
   "metadata": {},
   "source": [
    "### 3. Zapisz w pliku dataset_pca_n.csv wypadkowy zbiór danych jak poniżej: COMP1; COMP2; COMP3; COMP4; ...; COMPN; TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "joint-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 8)\n"
     ]
    }
   ],
   "source": [
    "features_new = ['COMP1', 'COMP2', 'COMP3', 'COMP4', 'COMP5', 'COMP6', 'COMP7', 'TARGET']\n",
    "dataset_new = np.append(reduced_new, target_new.reshape(569,1), axis=1)\n",
    "print(dataset_new.shape)\n",
    "df_new = pd.DataFrame(data = dataset_new, columns = features_new)\n",
    "df_new.to_csv(\"dataset_pca_n.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-shark",
   "metadata": {},
   "source": [
    "### 4. Oblicz wartość wariancji wyjaśnionej dla wygenerowanych składowych (ang. explained variance ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serial-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6027957151624883\n",
      "COMP1     8.745573\n",
      "COMP2     4.168247\n",
      "COMP3     1.718822\n",
      "COMP4     1.309718\n",
      "COMP5     1.021735\n",
      "COMP6     0.750052\n",
      "COMP7     0.537501\n",
      "TARGET    0.234177\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(reduced_new.var())\n",
    "print(df_new.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "included-corporation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43651014, 0.20804607, 0.08579006, 0.06537079, 0.05099695,\n",
       "       0.03743668, 0.02682781])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_new.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-bachelor",
   "metadata": {},
   "source": [
    "### 5. Skomentuj otrzymane rezultaty.\n",
    "Ponownie wariancja poszczególnych kolumn wzrosła, jednak przy kolejnych kolumnach te wzrosty są coraz mniejsze i bliższe wartości wariancji wyjaśnione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-culture",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "Wykonaj redukcję wymiarowości na podstawie dowolnego innego algorytmu a wynik zapisz w pliku dataset_algorithm.csv. Skomentuj otrzymane rezultaty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "phantom-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2).fit_transform(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "level-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 2)\n",
      "(569, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tsne.shape)\n",
    "features_new = ['COMP1', 'COMP2', 'TARGET']\n",
    "dataset_new = np.append(tsne, target_new.reshape(569,1), axis=1)\n",
    "print(dataset_new.shape)\n",
    "df_tsne = pd.DataFrame(data = dataset_new, columns = features_new)\n",
    "df_tsne.to_csv(\"dataset_tsne.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
